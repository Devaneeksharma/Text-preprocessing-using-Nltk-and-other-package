{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## FIT5196 Assessment 1: Text Processing\n",
    "Student Name: Devaneek sharma\n",
    "Student ID:29781965\n",
    "Date:14/04/2020\n",
    "\n",
    "Environment: Python 3.6.5 and Jupyter notebook Libraries used:\n",
    "            1. re (for regular expression, included in Anaconda Python 3.6)\n",
    "            2. PyPDF2 (PdfFileReader)\n",
    "            3. nlkt (probability,MWETokenizer tokenization and counter,porter stemmer)\n",
    "            4. itertool(for tool)\n",
    "            5. Collection(for counter).\n",
    "            6. sklearn.feature_extraction.text(CountVectorizer)\n",
    "            7. ast( for evaluating list as string)\n",
    "     \n",
    "***\n",
    "## Introduction:\n",
    "***\n",
    "- **Step-1:** Preprocessing the PDF file is done i.e Text is been extracted from the pdf file using \"PyPDF2\" and pdf reader then by using the \"re\" library is used to extract outcome and snopysis.\n",
    "- **Step-2:** Tokens are normalized to lowercase except the capital tokens appeared in the middle of a sentence/line.After preprocessing and cleaning of the data.   \n",
    "- **Step-3:** For word tokenization the following regular expression,\"\\w+(?:[-']\\w+)?\" and RegexpTokenizer library is used. \n",
    "- **Step-4:** In this step first 200 meaningful bigrams are take out by using nltk colocations and are included in the vocab.Then, by using \"mwetokenizer\" again tokenisation is been done and adding to the collocation dictionary.  \n",
    "- **Step-5:** The context-independent and context-dependent with the threshold set to %95 I.e words frequency>190 and word frequesncy<10 stop words are removed from the vocab dictionary. The stop words list from stopword provided by text is removed.In this step Unit wise frequency of the words is been counted I.e counting the individual words coming in how many unit. And for the words coming in more than 190 units and less than 10(rare word) from units are removed.\n",
    "- **Step-6:** Tokens are stemmed using the Porter stemmer. which remove commoner linguistics and inflexional endings\n",
    "- **Step-7:** Tokens with the length less than 3 should are removed from the vocab. as the porter stemmer decrease the the length of the words thus this step is done after stemming.\n",
    "- **Step-8:** Vocab is then sorted in the alphabatical order and idexing to vocab is given. our vocab is now ready for unit parse representation.\n",
    "- **Step-9:** Unit parse representation with the format is been created.\n",
    "           \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used in the module.\n",
    "- PyPDF2- used to convert the pdf to text\n",
    "- Collections- used for counter, which is used to count number of units.\n",
    "- nltk - tokenize for regex tokenizer, nltk.probability for frequency distribution, MWE tokenizer,porterstemmer.\n",
    "- sklearn.feature_extraction.text -for count vectorizer.\n",
    "- itertools- for chain which is used to get the list from the dictonary by chain looping.\n",
    "- Ast evaluate the the list as the string \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Text extraction from PDF and preprocessing the text.\n",
    "\n",
    "- PyPDF2, tool is used to extract the text from the file. After extraction text is been processed by using re library and regex is been written to filter unit,outcome and snopysis from the individual unit from the text.\n",
    "\n",
    "\n",
    "Finally dictionary-\"textDict\" is been created where the key is the unitcode and have the textual value of outsome and snopysis.\n",
    "thus this dictionary contains all the units with their value(outcome/synopsis).\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textDictionary={}\n",
    "snopList=[]\n",
    "outcomeList=[]\n",
    "# plese set the path in order to run the script.\n",
    "path = '29781965.pdf'\n",
    "# regex to take the output and synopsis, this this full block of text for each of the uit\n",
    "regexExtUnit= r\"([A-Z]{3}[0-9]{4})((?:\\n.*?)*])\"\n",
    "unitCodeRegex= r\"([A-Z]{3}[0-9]{4})\\n[A-Z]\"\n",
    "# to take outocme\n",
    "regOutcome=r\"(\\['.*[\\s\\S]*\\])\"\n",
    "# to take synopsis out.\n",
    "regSnopsis=r\"((?:.*\\n)*)\\['\"\n",
    "with open(path, 'rb') as f:\n",
    "    # read the pdf file\n",
    "    pdf = PdfFileReader(f)\n",
    "    count = PdfFileReader.numPages\n",
    "    out_dict = dict()\n",
    "    # read the no of pages \n",
    "    for i in range(63):\n",
    "        page = pdf.getPage(i)\n",
    "        text = page.extractText()\n",
    "        # using the regex to clear the data\n",
    "        reg = re.findall(regexExtUnit,text)\n",
    "        #regex to extract all the snopysis the and otucome\n",
    "        for reg in re.findall(regexExtUnit,text):\n",
    "            regValue= reg[1]\n",
    "            key= reg[0]\n",
    "            # regex to extract the snopysis and outcome.\n",
    "            snop=re.findall(regSnopsis,regValue)                \n",
    "            outcome= re.findall(regOutcome,regValue)\n",
    "            temp = (outcome[0].replace(\"\\n\",\" \"))\n",
    "            outcome_lines = \"\"\n",
    "            # here the list is been treating as string.\n",
    "            for x in ast.literal_eval(temp):\n",
    "                outcome_lines += (x)\n",
    "                # checking if the line end with'.' or not and the add '.' if not\n",
    "                if not x.endswith('.'):\n",
    "                    outcome_lines += \".\"\n",
    "            spout = snop[0].replace(\"\\n\",\" \")\n",
    "            spout = spout.strip()\n",
    "            spout += outcome_lines\n",
    "            textDictionary[key] = spout\n",
    "    \n",
    "    print(textDictionary)   \n",
    "            \n",
    "      \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Normalisation:\n",
    "- After the text is extracted and processed, function- \"lowerCaseSentence\" is been called that convert the forst words of the sentence and line to the lower case. from the \"synopsis\" delimiter used is '.' and for the outcome \"','\" is used as the delimiter.\n",
    "- Normalisation is difficult to done once we do the tokenisation, as it would be difficult to find which token is in start of the line and sentence hence Normalisation is soon done after prepocessing the text\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterating through the items of dictionary\n",
    "for unit, value in textDictionary.items():\n",
    "    linesUnit = \"\"\n",
    "    # spliting the line with delimiter '.'\n",
    "    for line in value.split('.'):\n",
    "        #cheking if the line is not null\n",
    "        if line is not None:\n",
    "            #split the line into words\n",
    "            wordList = line.split()\n",
    "            if len(wordList) != 0:\n",
    "                #changes the words to lower case\n",
    "                lowerWord = wordList[0].lower()\n",
    "                otherLine = \" \".join(wordList[1:])\n",
    "                linesUnit += \" \" + lowerWord + \" \" + otherLine\n",
    "    textDictionary[unit] = linesUnit\n",
    "print(textDictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Tokenisation\n",
    "- This is the helper function which is used to tokenise by using the regex tokenisation. expression r\"\\w+(?:[-']\\w+)?\"  has used to tokenise.\n",
    "- once the token is been tokenise the tokens we get are the unigrams. \n",
    "- tokeniseData- this dictionary is been created which will have all the tokens for the individual unit.\n",
    "\n",
    "\n",
    "#### Function: TokeniseText()\n",
    "function return \"unit id\", and \"tokens\" for that unit.\n",
    "#### parameters:\n",
    "- unitId(string)- unit id for retrive the value from dictionary.\n",
    "\n",
    "## Uni-gram:\n",
    "- Token generated after the tokenisation are called as unigrams.\n",
    "- chain.iterable is been used later to get the value of dictionary \"tokeniseData\" and then the total unigrams is been calculated.\n",
    "- removing the alphanumeric words if any.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenising the outcome and snopsis text \n",
    "def TokeniseText(unitId):\n",
    "    #print(unitId)\n",
    "    value = textDictionary[unitId]\n",
    "    # regex tokeniser used to get the tokens\n",
    "    tokenisedData = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    tokens=tokenisedData.tokenize(value)\n",
    "    return (unitId, tokens)\n",
    "\n",
    "# tokenise data for each unit\n",
    "tokeniseData = dict(TokeniseText(unitId) for unitId in textDictionary.keys())\n",
    "unitTokens = list(chain.from_iterable(tokeniseData.values()))\n",
    "print(\"Totale no of unigrams: \",len(unitTokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Bigram generation\n",
    "- Bigram has to be generated. we could find all the BI-gram by using bi-gram finder using the nltk. \n",
    "- Bigram is been generated before  context dependent and context independant stop words as some of uni-gram combining together could form the meaning word which could be lost if we are removing the context dependent and context independant first.\n",
    "- words length than <3 will cause a same thing we may lose some meaningful bigrams\n",
    "- same in the case of the removal of stemmer(as stemmer could inflexional the word). \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collocations finding the Bigram in the token.\n",
    "# generating the Bigram for the corelation\n",
    "bigramUnits = nltk.collocations.BigramAssocMeasures()\n",
    "# passing the token to bigrms tokeniser\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unitTokens)\n",
    "# by bigrams finder we could remove replace the bigrams.\n",
    "top200Bigrams = bigramFinder.nbest(bigramUnits.pmi, 200)\n",
    "\n",
    "#tokenising after bigram\n",
    "mwetokenizer = MWETokenizer(top200Bigrams)\n",
    "# adding the bigrms to the ourvocab dictionary.\n",
    "collocationUnitsDict =  dict((unitId, mwetokenizer.tokenize(info)) for unitId,info in tokeniseData.items())\n",
    "# getting list of our vocan dictionary.\n",
    "allWordsCollocation = list(chain.from_iterable(collocationUnitsDict.values()))\n",
    "print(\"After bigrams, length of our vocab: \", len(allWordsCollocation))\n",
    "print(top200Bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Removing Stop words\n",
    "- Now that we have unigrms and bigrams, we could start removing the stop words presented in the file 'stopwords_en.txt'.\n",
    "- Stop words is been read and then from the dictionary \"collocationUnitsDict\" which is out main dictionary, stopwords is been removed.\n",
    "- As, stemmer could change the wrord which could potential be a stopword thus stop words are removed before doing the stemmer.\n",
    "- After bigrams could form a ***Context Independent *** stop word thus after bigram, stop words is been removed.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stop words\n",
    "stopwords = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    #tokenizedUnits = {}\n",
    "\n",
    "# removing the context independent stopwords from the out vocb dictionary.\n",
    "for unitid in textDict.keys():\n",
    "    # removing stop words from the dictionary\n",
    "    collocationUnitsDict[unitid] = [wrd for wrd in collocationUnitsDict[unitid] if wrd not in stopwords]\n",
    "# list of values of the vocab dictionary.\n",
    "aftRemovingStopWords = list(chain.from_iterable(collocationUnitsDict.values()))\n",
    "print(\"After removing context Independent stop words our vocab dictionay length: \",len(aftRemovingStopWords))\n",
    "fd_3 = FreqDist(aftRemovingStopWords)\n",
    "fd_3.most_common(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Removing context dependant stopwords \n",
    "- After the context Independant, Context dependant stops words with threshold of greter than 190(95%) and less than 10(5%) is been removed.\n",
    "- Here unit wise the frequency of the words is been calculated. I.e how many times individual word is coming in diffrent units.\n",
    "- For the words more than 190 and less than 10 is been removed.\n",
    "- As the stemmer change the words thus we need to remove the context dependant stopords before it. \n",
    "- In our data analysis if the word is coming in more than 190 it means that it is context dependant and e cant derive anything from it. same incase for words which is coming in less than 10 words, as we can't derive anything from them too.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the word frequency for idividual units-context dependant\n",
    "dictUnitCount={}\n",
    "# iterating through the key of the dictionary\n",
    "for k,v in collocationUnitsDict.items():\n",
    "    # used to count the no of individual words for each unit. \n",
    "    counter = Counter(collocationUnitsDict[k])\n",
    "    # ading the count of word frequency for each unit to the new dictionary\n",
    "    dictUnitCount[k]=counter\n",
    "# getting the values for the list dicUnitcount\n",
    "dic = list(chain.from_iterable(dictUnitCount.values()))\n",
    "# calculate the frequency distribution the words. which will give how many times a single words come in different units.\n",
    "freqDistriContDep= FreqDist(dic)\n",
    "#freqDistriContDep.most_common(50)\n",
    "#print(len(aftRemovingStopWords))\n",
    "\n",
    "#removing the words with the count <10 or >190 and \n",
    "for unitid in collocationUnitsDict.keys():\n",
    "    # iterating through the new dictionary containing freq distibution. \n",
    "    for k,v in freqDistriContDep.items():\n",
    "        if v<10 or v>190:\n",
    "            # removing the context dependant stopwords.\n",
    "            collocationUnitsDict[unitid] = [wrd for wrd in collocationUnitsDict[unitid] if wrd!=k]\n",
    "\n",
    "print(\"Length of vocab after removing Context dependant stopwords:\",(len(list(chain.from_iterable(collocationUnitsDict.values())))))                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# # Removing the words with the length less than 3.\n",
    "Words with the length than 3 is been removed. this is been done before stemming as non-meaningful words after stemming could form different meaning and will not help in data analysis. thus words lenth less than 3 should be removed before stemming the words.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removing the words having length less than 3:\n",
    "for unitid in collocationUnitsDict.keys():\n",
    "    collocationUnitsDict[unitid] = [wrd for wrd in collocationUnitsDict[unitid] if len(wrd)>=3]\n",
    "               \n",
    "#print(\"Length of vocab after removing Context dependant stopwords:\",(len(list(chain.from_iterable(collocationUnitsDict.values())))))                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Extracting the Nouns.\n",
    "Nouns words in the dictionary is been extracted from out vacab and not to be used in stemming. As for stemmer it changes the words meaning there is not point of stemming the noum.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of vacab dictionary is been created for extracting the Nouns\n",
    "listValueVocabDict = list(chain.from_iterable(collocationUnitsDict.values()))\n",
    "# passing the vocab list for noun Part-of-Speeching tagging.\n",
    "nounVocab = nltk.tag.pos_tag(listValueVocabDict)\n",
    "#all the nouns is been created.\n",
    "allNouns = [w for w,t in nounVocab if t.startswith('NN')]\n",
    "allNouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Porter Stemmer\n",
    "- Now that we have removed stop words, \n",
    "- Many variation of the word could contain the same meaning. Thus it is important to use stemmer and remove those words. As they would have no impact in the data analysis. \n",
    "- Nouns are not stemmed here.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of the Porter stemmer\n",
    "# porter stemmer object\n",
    "ps = PorterStemmer()\n",
    "#iterate through the vocab dictionary items\n",
    "for key,value in collocationUnitsDict.items():\n",
    "    stemOutputLst=[]\n",
    "    # iterate throught the each unit list.\n",
    "    for val in value:\n",
    "        # checking if the words are nouns, if not stemmed them\n",
    "        if val not in allNouns:\n",
    "            # stemming the text\n",
    "            stemOutput= ps.stem(val)\n",
    "            stemOutputLst.append(stemOutput)\n",
    "        else:\n",
    "            stemOutputLst.append(val)\n",
    "    # updating the vocab dictionary with the new values.        \n",
    "    collocationUnitsDict[key] =stemOutputLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "***\n",
    "#  Data feature generation and Vocab  Dictionary Sorting.\n",
    "list of the dictionary values is been created and then they are sorted in alphabatical order. \n",
    "Unit are total- 200 and have vocab 253 which would be out column in parse representation.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse representation\n",
    "listValueVocabDict = list(chain.from_iterable(collocationUnitsDict.values()))\n",
    "print(len(set(listValueVocabDict)))\n",
    "print(set(listValueVocabDict))\n",
    "vocabFinal=[]\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "# data feature of the vocab\n",
    "dataFeatures = vectorizer.fit_transform([' '.join(value) for value in collocationUnitsDict.values()])\n",
    "#Final vocab of the units, vocab is sorted in the alphabatical order here.\n",
    "vocabFinal= vectorizer.get_feature_names()\n",
    "print(\"Data shape(Units,Vocab):\",dataFeatures.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Indexing of the Vocabulary\n",
    "This is the final step for vocab generation. Index is been added to each word and thus final vocab dictionary is been generated.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterating through final vocab and addinf index.\n",
    "dictWordIndex={}\n",
    "#iterate over the vocab list.\n",
    "for i in range(len(vocabFinal)):\n",
    "    # adding the idex to the vocab dictionary\n",
    "    dictWordIndex[vocabFinal[i]]= i\n",
    "\n",
    "# creating the file object and opening it in append mode\n",
    "with open('29781965_vocab.txt',\"a+\") as infile:\n",
    "    for k,val in dictWordIndex.items():\n",
    "        strVcab= k+\":\"+ str(val)+\"\\n\"\n",
    "        infile.write(strVcab)\n",
    "print(\"Dictionary with the Indexes:\",dictWordIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Creating the parse Representations for units\n",
    "Parse represenation for the units is been created, for the vocab which is created by count.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate the parse representation\n",
    "finalOutputList=[]\n",
    "r=0\n",
    "c=0\n",
    "vcabForParse=[]\n",
    "vcabForParse= list(dictWordIndex.values())\n",
    "\n",
    "units=list(collocationUnitsDict.keys())\n",
    "for r in range(len(units)):\n",
    "    strFinalOutput=units[r] \n",
    "    for c in range(len(vocabFinal)):\n",
    "        strFinalOutput=strFinalOutput +\",\"+str(vcabForParse[c])+\":\"+ str(dataFeatures.toarray()[r][c])+\" \"\n",
    "    finalOutputList.append(strFinalOutput)\n",
    "\n",
    "# creating the file object and opening it in append mode\n",
    "with open('29781965_countVec.txt',\"a+\") as infile:\n",
    "    for val in finalOutputList:\n",
    "        print(val)\n",
    "        infile.write(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before wrangling statistics\n",
    "words = unitTokens\n",
    "len(words)\n",
    "vocab = set(words)\n",
    "lexicalDiversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexicalDiversity)\n",
    "print(\"hello\")\n",
    "listFinalSummary = list(chain.from_iterable(collocationUnitsDict.values()))\n",
    "wordAF=len(listFinalSummary)\n",
    "# After wrangling statistics\n",
    "len(listFinalSummary)\n",
    "vocabAF = set(listFinalSummary)\n",
    "lexicalDiversityAF = len(listFinalSummary)/len(vocabAF)\n",
    "print (\"Vocabulary size: \",len(vocabAF),\"\\nTotal number of tokens: \" ,wordAF,\n",
    "\"\\nLexical diversity: \", lexicalDiversityAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summary of the data wrangling is been represented below.\n",
    "\n",
    "######################### Text Statistics Before Wrangling ##################################\n",
    "\n",
    "Totale no of Words:  31649<br>\n",
    "Vocabulary size:  4284<br> \n",
    "Lexical diversity:  7.387721755368815<br>\n",
    "\n",
    "######################### Final Text Statistics After Wranling ##################################\n",
    "\n",
    "Total number of words:12116 <br>\n",
    "Total number of vocabs: 310<br>\n",
    "Lexical diversity is : 39.83 %<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
